{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1423,"sourceType":"datasetVersion","datasetId":747}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shekharbanerjee/skb-quora-question-pairs?scriptVersionId=180942238\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-05T12:32:12.939474Z","iopub.execute_input":"2024-05-05T12:32:12.94064Z","iopub.status.idle":"2024-05-05T12:32:14.158959Z","shell.execute_reply.started":"2024-05-05T12:32:12.9406Z","shell.execute_reply":"2024-05-05T12:32:14.158094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk , string\n\nimport re\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = stopwords.words('english')\n\nnltk.download('wordnet')\n\nfrom bs4 import BeautifulSoup\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:14.160846Z","iopub.execute_input":"2024-05-05T12:32:14.161565Z","iopub.status.idle":"2024-05-05T12:32:16.883437Z","shell.execute_reply.started":"2024-05-05T12:32:14.161533Z","shell.execute_reply":"2024-05-05T12:32:16.882232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/question-pairs-dataset/questions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:16.88469Z","iopub.execute_input":"2024-05-05T12:32:16.885216Z","iopub.status.idle":"2024-05-05T12:32:19.144465Z","shell.execute_reply.started":"2024-05-05T12:32:16.885188Z","shell.execute_reply":"2024-05-05T12:32:19.143152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.146589Z","iopub.execute_input":"2024-05-05T12:32:19.147139Z","iopub.status.idle":"2024-05-05T12:32:19.155048Z","shell.execute_reply.started":"2024-05-05T12:32:19.147107Z","shell.execute_reply":"2024-05-05T12:32:19.153806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.156361Z","iopub.execute_input":"2024-05-05T12:32:19.157646Z","iopub.status.idle":"2024-05-05T12:32:19.180966Z","shell.execute_reply.started":"2024-05-05T12:32:19.157613Z","shell.execute_reply":"2024-05-05T12:32:19.179808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['id','qid1','qid2'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.182133Z","iopub.execute_input":"2024-05-05T12:32:19.182458Z","iopub.status.idle":"2024-05-05T12:32:19.216342Z","shell.execute_reply.started":"2024-05-05T12:32:19.182431Z","shell.execute_reply":"2024-05-05T12:32:19.215286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['is_duplicate'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.217522Z","iopub.execute_input":"2024-05-05T12:32:19.21815Z","iopub.status.idle":"2024-05-05T12:32:19.241673Z","shell.execute_reply.started":"2024-05-05T12:32:19.21812Z","shell.execute_reply":"2024-05-05T12:32:19.240601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['is_duplicate'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.243025Z","iopub.execute_input":"2024-05-05T12:32:19.2436Z","iopub.status.idle":"2024-05-05T12:32:19.256351Z","shell.execute_reply.started":"2024-05-05T12:32:19.243566Z","shell.execute_reply":"2024-05-05T12:32:19.255037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df[df['is_duplicate'] == 1].sample(frac = 0.1) ,df[df['is_duplicate'] == 0].sample(frac = 0.05)],axis=0).sample(frac=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.257738Z","iopub.execute_input":"2024-05-05T12:32:19.258645Z","iopub.status.idle":"2024-05-05T12:32:19.374146Z","shell.execute_reply.started":"2024-05-05T12:32:19.258607Z","shell.execute_reply":"2024-05-05T12:32:19.372945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.377775Z","iopub.execute_input":"2024-05-05T12:32:19.378142Z","iopub.status.idle":"2024-05-05T12:32:19.384403Z","shell.execute_reply.started":"2024-05-05T12:32:19.378115Z","shell.execute_reply":"2024-05-05T12:32:19.383098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['is_duplicate'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.385579Z","iopub.execute_input":"2024-05-05T12:32:19.386155Z","iopub.status.idle":"2024-05-05T12:32:19.398939Z","shell.execute_reply.started":"2024-05-05T12:32:19.386125Z","shell.execute_reply":"2024-05-05T12:32:19.397855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['is_duplicate'].value_counts().plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.400335Z","iopub.execute_input":"2024-05-05T12:32:19.400781Z","iopub.status.idle":"2024-05-05T12:32:19.663831Z","shell.execute_reply.started":"2024-05-05T12:32:19.400733Z","shell.execute_reply":"2024-05-05T12:32:19.662671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The dataset is imbalanced**  \nBut as the adatset is large , we can handle this by splitting dataset in a stratified fashion","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.665335Z","iopub.execute_input":"2024-05-05T12:32:19.665752Z","iopub.status.idle":"2024-05-05T12:32:19.695395Z","shell.execute_reply.started":"2024-05-05T12:32:19.665723Z","shell.execute_reply":"2024-05-05T12:32:19.694311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.696679Z","iopub.execute_input":"2024-05-05T12:32:19.697042Z","iopub.status.idle":"2024-05-05T12:32:19.719284Z","shell.execute_reply.started":"2024-05-05T12:32:19.697015Z","shell.execute_reply":"2024-05-05T12:32:19.718054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.720746Z","iopub.execute_input":"2024-05-05T12:32:19.721599Z","iopub.status.idle":"2024-05-05T12:32:19.7444Z","shell.execute_reply.started":"2024-05-05T12:32:19.721567Z","shell.execute_reply":"2024-05-05T12:32:19.743056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.745846Z","iopub.execute_input":"2024-05-05T12:32:19.746179Z","iopub.status.idle":"2024-05-05T12:32:19.767754Z","shell.execute_reply.started":"2024-05-05T12:32:19.746154Z","shell.execute_reply":"2024-05-05T12:32:19.76651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Repeated questions\n\nqid = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nprint('Number of unique questions : ',np.unique(qid).shape[0])\nx = qid.duplicated()\nprint('Number of questions getting repeated : ',x.sum())","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.769355Z","iopub.execute_input":"2024-05-05T12:32:19.769679Z","iopub.status.idle":"2024-05-05T12:32:19.802301Z","shell.execute_reply.started":"2024-05-05T12:32:19.769648Z","shell.execute_reply":"2024-05-05T12:32:19.801014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Repeated questions histogram\n\nplt.hist(qid.value_counts().values,bins=160)\nplt.yscale('log')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:19.803721Z","iopub.execute_input":"2024-05-05T12:32:19.804089Z","iopub.status.idle":"2024-05-05T12:32:20.710803Z","shell.execute_reply.started":"2024-05-05T12:32:19.804062Z","shell.execute_reply":"2024-05-05T12:32:20.709593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(20)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:20.712272Z","iopub.execute_input":"2024-05-05T12:32:20.712714Z","iopub.status.idle":"2024-05-05T12:32:20.73025Z","shell.execute_reply.started":"2024-05-05T12:32:20.712678Z","shell.execute_reply":"2024-05-05T12:32:20.729145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Steps for going ahead with the data :\n\n1. Preprocessing\n    * Remove Punctuation\n    * Remove stopwords\n    * Remove contractions\n","metadata":{}},{"cell_type":"code","source":"def preprocess(q):\n    \n    q = str(q).lower().strip()\n    \n    # Replace certain special characters with their string equivalents\n    q = q.replace('%', ' percent')\n    q = q.replace('$', ' dollar ')\n    q = q.replace('₹', ' rupee ')\n    q = q.replace('€', ' euro ')\n    q = q.replace('@', ' at ')\n    \n    # The pattern '[math]' appears around 900 times in the whole dataset.\n    q = q.replace('[math]', '')\n    \n    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n    q = q.replace(',000,000,000 ', 'b ')\n    q = q.replace(',000,000 ', 'm ')\n    q = q.replace(',000 ', 'k ')\n    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n    \n    # Decontracting words\n    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n    # https://stackoverflow.com/a/19794953\n    contractions = { \n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"can not\",\n    \"can't've\": \"can not have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n    }\n\n    q_decontracted = []\n\n    for word in q.split():\n        if word in contractions:\n            word = contractions[word]\n\n        q_decontracted.append(word)\n\n    q = ' '.join(q_decontracted)\n    q = q.replace(\"'ve\", \" have\")\n    q = q.replace(\"n't\", \" not\")\n    q = q.replace(\"'re\", \" are\")\n    q = q.replace(\"'ll\", \" will\")\n    \n    # Removing HTML tags\n    q = BeautifulSoup(q)\n    q = q.get_text()\n    \n    # Remove punctuations\n    pattern = re.compile('\\W')\n    q = re.sub(pattern, ' ', q).strip()\n\n    \n    return q","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:20.732105Z","iopub.execute_input":"2024-05-05T12:32:20.73282Z","iopub.status.idle":"2024-05-05T12:32:20.754352Z","shell.execute_reply.started":"2024-05-05T12:32:20.732781Z","shell.execute_reply":"2024-05-05T12:32:20.753369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['question1'] = df['question1'].apply(preprocess)\ndf['question2'] = df['question2'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:20.75553Z","iopub.execute_input":"2024-05-05T12:32:20.756466Z","iopub.status.idle":"2024-05-05T12:32:34.760538Z","shell.execute_reply.started":"2024-05-05T12:32:20.756434Z","shell.execute_reply":"2024-05-05T12:32:34.759347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(20)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-05T12:32:34.762811Z","iopub.execute_input":"2024-05-05T12:32:34.763633Z","iopub.status.idle":"2024-05-05T12:32:34.778822Z","shell.execute_reply.started":"2024-05-05T12:32:34.763597Z","shell.execute_reply":"2024-05-05T12:32:34.77768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['q1_len'] = df['question1'].apply(lambda x : len(x.split()))\ndf['q2_len'] = df['question2'].apply(lambda x : len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:34.781307Z","iopub.execute_input":"2024-05-05T12:32:34.78227Z","iopub.status.idle":"2024-05-05T12:32:34.86107Z","shell.execute_reply.started":"2024-05-05T12:32:34.782236Z","shell.execute_reply":"2024-05-05T12:32:34.859568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['q1_num_words'] = df['question1'].apply(lambda x : len(x.split(' ')))\ndf['q2_num_words'] = df['question2'].apply(lambda x : len(x.split(' ')))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:34.862779Z","iopub.execute_input":"2024-05-05T12:32:34.863623Z","iopub.status.idle":"2024-05-05T12:32:34.938641Z","shell.execute_reply.started":"2024-05-05T12:32:34.863571Z","shell.execute_reply":"2024-05-05T12:32:34.93762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def common_words(row):\n    \n    w1 = set(row['question1'].split(' '))\n    w2 = set(row['question2'].split(' '))\n    \n    return len(w1 & w2)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:34.940097Z","iopub.execute_input":"2024-05-05T12:32:34.940448Z","iopub.status.idle":"2024-05-05T12:32:34.945818Z","shell.execute_reply.started":"2024-05-05T12:32:34.940421Z","shell.execute_reply":"2024-05-05T12:32:34.944841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['common_words'] = df.apply(common_words, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:34.947222Z","iopub.execute_input":"2024-05-05T12:32:34.947644Z","iopub.status.idle":"2024-05-05T12:32:35.420293Z","shell.execute_reply.started":"2024-05-05T12:32:34.947607Z","shell.execute_reply":"2024-05-05T12:32:35.419223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:35.421837Z","iopub.execute_input":"2024-05-05T12:32:35.422276Z","iopub.status.idle":"2024-05-05T12:32:35.438576Z","shell.execute_reply.started":"2024-05-05T12:32:35.422241Z","shell.execute_reply":"2024-05-05T12:32:35.437103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def total_words(row):\n    \n    w1 = len(row['question1'].split(' '))\n    w2 = len(row['question2'].split(' '))\n    \n    return w1 + w2","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:35.447819Z","iopub.execute_input":"2024-05-05T12:32:35.448481Z","iopub.status.idle":"2024-05-05T12:32:35.454378Z","shell.execute_reply.started":"2024-05-05T12:32:35.448417Z","shell.execute_reply":"2024-05-05T12:32:35.453148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['total_words'] = df.apply(total_words, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:35.456113Z","iopub.execute_input":"2024-05-05T12:32:35.456648Z","iopub.status.idle":"2024-05-05T12:32:35.835Z","shell.execute_reply.started":"2024-05-05T12:32:35.456608Z","shell.execute_reply":"2024-05-05T12:32:35.833886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['word_share'] = round(df['common_words']/df['total_words'],2)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:35.83631Z","iopub.execute_input":"2024-05-05T12:32:35.836755Z","iopub.status.idle":"2024-05-05T12:32:35.843934Z","shell.execute_reply.started":"2024-05-05T12:32:35.836719Z","shell.execute_reply":"2024-05-05T12:32:35.842781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_token_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    SAFE_DIV = 0.0001\n    \n    STOP_WORDS = stopwords.words('english')\n    \n    token_features = [0.0] * 8\n    \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0 :\n        return token_features\n    \n    # Non stopwords\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    # Stopwords\n    \n    q1_stopwords = set([word for word in q1_tokens if word  in STOP_WORDS])\n    q2_stopwords = set([word for word in q2_tokens if word  in STOP_WORDS])\n    \n    \n    # Common Word Count\n    \n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Common Stopword Count\n    \n    common_stopword_count = len(q1_stopwords.intersection(q2_stopwords))\n    \n    # Get the common Tokens from Question pair\n    \n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words) , len(q2_words))+ SAFE_DIV) \n    token_features[1] = common_word_count / (max(len(q1_words) , len(q2_words))+ SAFE_DIV)\n    token_features[2] = common_stopword_count / (min(len(q1_stopwords) , len(q2_stopwords))+ SAFE_DIV)\n    token_features[3] = common_stopword_count / (max(len(q1_stopwords) , len(q2_stopwords))+ SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens) , len(q2_tokens))+ SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens) , len(q2_tokens)) + SAFE_DIV)\n    \n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    \n    return token_features\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:35.845723Z","iopub.execute_input":"2024-05-05T12:32:35.846418Z","iopub.status.idle":"2024-05-05T12:32:35.859431Z","shell.execute_reply.started":"2024-05-05T12:32:35.846376Z","shell.execute_reply":"2024-05-05T12:32:35.858351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_features = df.apply(fetch_token_features,axis=1)\n\ndf['cwc_min'] = list(map(lambda x : x[0] , token_features))\ndf['cwc_max'] = list(map(lambda x : x[1] , token_features))\ndf['csc_min'] = list(map(lambda x : x[2] , token_features))\ndf['csc_max'] = list(map(lambda x : x[3] , token_features))\ndf['ctc_min'] = list(map(lambda x : x[4] , token_features))\ndf['ctc_max'] = list(map(lambda x : x[5] , token_features))\ndf['last_word_eq'] = list(map(lambda x : x[6] , token_features))\ndf['first_word_eq'] = list(map(lambda x : x[7] , token_features))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:35.860829Z","iopub.execute_input":"2024-05-05T12:32:35.86122Z","iopub.status.idle":"2024-05-05T12:32:42.964658Z","shell.execute_reply.started":"2024-05-05T12:32:35.861192Z","shell.execute_reply":"2024-05-05T12:32:42.963369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:42.966164Z","iopub.execute_input":"2024-05-05T12:32:42.966626Z","iopub.status.idle":"2024-05-05T12:32:42.989136Z","shell.execute_reply.started":"2024-05-05T12:32:42.966588Z","shell.execute_reply":"2024-05-05T12:32:42.987846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install Distance","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:42.990968Z","iopub.execute_input":"2024-05-05T12:32:42.9916Z","iopub.status.idle":"2024-05-05T12:32:58.943348Z","shell.execute_reply.started":"2024-05-05T12:32:42.99156Z","shell.execute_reply":"2024-05-05T12:32:58.94189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport distance\n\ndef fetch_length_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    length_features = [0.0]*3\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n    \n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1) if strs else 0\n    \n    return length_features\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:58.945145Z","iopub.execute_input":"2024-05-05T12:32:58.94555Z","iopub.status.idle":"2024-05-05T12:32:58.966603Z","shell.execute_reply.started":"2024-05-05T12:32:58.945517Z","shell.execute_reply":"2024-05-05T12:32:58.964989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length_features = df.apply(fetch_length_features, axis=1)\n\ndf['abs_len_diff'] = list(map(lambda x: x[0], length_features))\ndf['mean_len'] = list(map(lambda x: x[1], length_features))\ndf['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:32:58.968166Z","iopub.execute_input":"2024-05-05T12:32:58.968542Z","iopub.status.idle":"2024-05-05T12:33:20.293497Z","shell.execute_reply.started":"2024-05-05T12:32:58.968506Z","shell.execute_reply":"2024-05-05T12:33:20.292251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:33:20.294956Z","iopub.execute_input":"2024-05-05T12:33:20.295317Z","iopub.status.idle":"2024-05-05T12:33:20.323985Z","shell.execute_reply.started":"2024-05-05T12:33:20.29529Z","shell.execute_reply":"2024-05-05T12:33:20.323167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fuzzy Features\nfrom fuzzywuzzy import fuzz\n\ndef fetch_fuzzy_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    fuzzy_features = [0.0]*4\n    \n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features\n\nfuzzy_features = df.apply(fetch_fuzzy_features, axis=1)\n\n# Creating new feature columns for fuzzy features\ndf['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\ndf['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\ndf['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\ndf['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:33:20.325058Z","iopub.execute_input":"2024-05-05T12:33:20.325651Z","iopub.status.idle":"2024-05-05T12:33:20.338327Z","shell.execute_reply.started":"2024-05-05T12:33:20.325621Z","shell.execute_reply":"2024-05-05T12:33:20.337102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:32.251977Z","iopub.execute_input":"2024-05-05T12:34:32.25228Z","iopub.status.idle":"2024-05-05T12:34:32.276716Z","shell.execute_reply.started":"2024-05-05T12:34:32.252256Z","shell.execute_reply":"2024-05-05T12:34:32.275747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'is_duplicate']],hue='is_duplicate')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:10:58.339052Z","iopub.execute_input":"2024-05-05T12:10:58.339388Z","iopub.status.idle":"2024-05-05T12:12:19.269686Z","shell.execute_reply.started":"2024-05-05T12:10:58.339359Z","shell.execute_reply":"2024-05-05T12:12:19.268727Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df[['ctc_max', 'cwc_max', 'csc_max', 'is_duplicate']],hue='is_duplicate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df[['last_word_eq', 'first_word_eq', 'is_duplicate']],hue='is_duplicate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df[['fuzz_ratio', 'fuzz_partial_ratio','token_sort_ratio','token_set_ratio', 'is_duplicate']],hue='is_duplicate')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:13:35.173505Z","iopub.status.idle":"2024-05-05T12:13:35.174647Z","shell.execute_reply.started":"2024-05-05T12:13:35.174318Z","shell.execute_reply":"2024-05-05T12:13:35.174347Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df[['mean_len', 'abs_len_diff','longest_substr_ratio', 'is_duplicate']],hue='is_duplicate')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:13:35.176072Z","iopub.status.idle":"2024-05-05T12:13:35.176473Z","shell.execute_reply.started":"2024-05-05T12:13:35.176279Z","shell.execute_reply":"2024-05-05T12:13:35.176295Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df[['fuzz_ratio', 'fuzz_partial_ratio','token_sort_ratio','token_set_ratio', 'is_duplicate']],hue='is_duplicate')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:13:35.178134Z","iopub.status.idle":"2024-05-05T12:13:35.178717Z","shell.execute_reply.started":"2024-05-05T12:13:35.178522Z","shell.execute_reply":"2024-05-05T12:13:35.17854Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# merge texts\nquestions = list(df['question1']) + list(df['question2'])\n\ncv = TfidfVectorizer(max_features=3000)\nq1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:32.278112Z","iopub.execute_input":"2024-05-05T12:34:32.278418Z","iopub.status.idle":"2024-05-05T12:34:34.492357Z","shell.execute_reply.started":"2024-05-05T12:34:32.278394Z","shell.execute_reply":"2024-05-05T12:34:34.491327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['question1', 'question2'],axis=1)\n\nfinal_df = pd.concat([df, pd.DataFrame(q1_arr, index=df.index), pd.DataFrame(q2_arr, index= df.index)], axis=1)\nprint(final_df.shape)\nfinal_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:34.493576Z","iopub.execute_input":"2024-05-05T12:34:34.494388Z","iopub.status.idle":"2024-05-05T12:34:37.507251Z","shell.execute_reply.started":"2024-05-05T12:34:34.494357Z","shell.execute_reply":"2024-05-05T12:34:37.506459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del questions , q1_arr , q2_arr , df","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:37.50842Z","iopub.execute_input":"2024-05-05T12:34:37.509257Z","iopub.status.idle":"2024-05-05T12:34:37.568997Z","shell.execute_reply.started":"2024-05-05T12:34:37.509228Z","shell.execute_reply":"2024-05-05T12:34:37.567957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.columns = [str(i) for i in final_df.columns]\nX = final_df.drop(['is_duplicate'],axis=1)\ny = final_df['is_duplicate']\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:37.570123Z","iopub.execute_input":"2024-05-05T12:34:37.571045Z","iopub.status.idle":"2024-05-05T12:34:38.034891Z","shell.execute_reply.started":"2024-05-05T12:34:37.571013Z","shell.execute_reply":"2024-05-05T12:34:38.033932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del final_df","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:38.03601Z","iopub.execute_input":"2024-05-05T12:34:38.036341Z","iopub.status.idle":"2024-05-05T12:34:38.040934Z","shell.execute_reply.started":"2024-05-05T12:34:38.036314Z","shell.execute_reply":"2024-05-05T12:34:38.039977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=101)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:38.042085Z","iopub.execute_input":"2024-05-05T12:34:38.042431Z","iopub.status.idle":"2024-05-05T12:34:39.307174Z","shell.execute_reply.started":"2024-05-05T12:34:38.042398Z","shell.execute_reply":"2024-05-05T12:34:39.305955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:39.308652Z","iopub.execute_input":"2024-05-05T12:34:39.309027Z","iopub.status.idle":"2024-05-05T12:34:39.316855Z","shell.execute_reply.started":"2024-05-05T12:34:39.309Z","shell.execute_reply":"2024-05-05T12:34:39.315781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:34:39.318267Z","iopub.execute_input":"2024-05-05T12:34:39.318752Z","iopub.status.idle":"2024-05-05T12:34:39.326862Z","shell.execute_reply.started":"2024-05-05T12:34:39.318724Z","shell.execute_reply":"2024-05-05T12:34:39.325902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:37:15.693209Z","iopub.execute_input":"2024-05-05T12:37:15.693695Z","iopub.status.idle":"2024-05-05T12:38:06.196133Z","shell.execute_reply.started":"2024-05-05T12:37:15.693663Z","shell.execute_reply":"2024-05-05T12:38:06.194933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train.values,y_train.values)\ny_pred1 = xgb.predict(X_test.values)\naccuracy_score(y_test.values,y_pred1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:06.198201Z","iopub.execute_input":"2024-05-05T12:38:06.19913Z","iopub.status.idle":"2024-05-05T12:38:55.633329Z","shell.execute_reply.started":"2024-05-05T12:38:06.199097Z","shell.execute_reply":"2024-05-05T12:38:55.632161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , classification_report","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:55.634747Z","iopub.execute_input":"2024-05-05T12:38:55.635709Z","iopub.status.idle":"2024-05-05T12:38:55.640919Z","shell.execute_reply.started":"2024-05-05T12:38:55.635667Z","shell.execute_reply":"2024-05-05T12:38:55.639841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for random forest model\nconfusion_matrix(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:55.643754Z","iopub.execute_input":"2024-05-05T12:38:55.644483Z","iopub.status.idle":"2024-05-05T12:38:55.659349Z","shell.execute_reply.started":"2024-05-05T12:38:55.644451Z","shell.execute_reply":"2024-05-05T12:38:55.658199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for xgboost model\nconfusion_matrix(y_test,y_pred1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:55.661132Z","iopub.execute_input":"2024-05-05T12:38:55.661573Z","iopub.status.idle":"2024-05-05T12:38:55.673186Z","shell.execute_reply.started":"2024-05-05T12:38:55.661537Z","shell.execute_reply":"2024-05-05T12:38:55.672114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:55.674791Z","iopub.execute_input":"2024-05-05T12:38:55.675137Z","iopub.status.idle":"2024-05-05T12:38:55.704851Z","shell.execute_reply.started":"2024-05-05T12:38:55.67511Z","shell.execute_reply":"2024-05-05T12:38:55.703749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred1))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:55.706357Z","iopub.execute_input":"2024-05-05T12:38:55.707093Z","iopub.status.idle":"2024-05-05T12:38:55.734821Z","shell.execute_reply.started":"2024-05-05T12:38:55.707054Z","shell.execute_reply":"2024-05-05T12:38:55.733693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_common_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n    return len(w1 & w2)\n\n\ndef test_total_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n    return (len(w1) + len(w2))\n\n\ndef test_fetch_token_features(q1,q2):\n    \n    SAFE_DIV = 0.0001 \n\n    STOP_WORDS = stopwords.words(\"english\")\n    \n    token_features = [0.0]*8\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    return token_features\n\n\ndef test_fetch_length_features(q1,q2):\n    \n    length_features = [0.0]*3\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n    \n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n    \n    return length_features\n\ndef test_fetch_fuzzy_features(q1,q2):\n    \n    fuzzy_features = [0.0]*4\n    \n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:36:04.769546Z","iopub.execute_input":"2024-05-05T12:36:04.770426Z","iopub.status.idle":"2024-05-05T12:36:04.789954Z","shell.execute_reply.started":"2024-05-05T12:36:04.770383Z","shell.execute_reply":"2024-05-05T12:36:04.789068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def query_point_creator(q1,q2):\n    \n    input_query = []\n    \n    # preprocess\n    q1 = preprocess(q1)\n    q2 = preprocess(q2)\n    \n    # fetch basic features\n    input_query.append(len(q1))\n    input_query.append(len(q2))\n    \n    input_query.append(len(q1.split(\" \")))\n    input_query.append(len(q2.split(\" \")))\n    \n    input_query.append(test_common_words(q1,q2))\n    input_query.append(test_total_words(q1,q2))\n    input_query.append(round(test_common_words(q1,q2)/test_total_words(q1,q2),2))\n    \n    # fetch token features\n    token_features = test_fetch_token_features(q1,q2)\n    input_query.extend(token_features)\n    \n    # fetch length based features\n    length_features = test_fetch_length_features(q1,q2)\n    input_query.extend(length_features)\n    \n    # fetch fuzzy features\n    fuzzy_features = test_fetch_fuzzy_features(q1,q2)\n    input_query.extend(fuzzy_features)\n    \n    # bow feature for q1\n    q1_bow = cv.transform([q1]).toarray()\n    \n    # bow feature for q2\n    q2_bow = cv.transform([q2]).toarray()\n    \n    \n    \n    return np.hstack((np.array(input_query).reshape(1,22),q1_bow,q2_bow))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:36:04.791497Z","iopub.execute_input":"2024-05-05T12:36:04.79196Z","iopub.status.idle":"2024-05-05T12:36:04.807197Z","shell.execute_reply.started":"2024-05-05T12:36:04.791925Z","shell.execute_reply":"2024-05-05T12:36:04.805841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nq1 = 'Where is the capital of India?'\nq2 = 'What is the current capital of Pakistan?'\nq3 = 'Which city serves as the capital of Pakistan?'\nq4 = 'What is the business capital of India?'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:55.736062Z","iopub.execute_input":"2024-05-05T12:38:55.736383Z","iopub.status.idle":"2024-05-05T12:38:55.74126Z","shell.execute_reply.started":"2024-05-05T12:38:55.736358Z","shell.execute_reply":"2024-05-05T12:38:55.740172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.predict(query_point_creator(q2,q3))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:39:12.213945Z","iopub.execute_input":"2024-05-05T12:39:12.216202Z","iopub.status.idle":"2024-05-05T12:39:12.232773Z","shell.execute_reply.started":"2024-05-05T12:39:12.216156Z","shell.execute_reply":"2024-05-05T12:39:12.231574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.predict(query_point_creator(q2,q4))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:40:17.553065Z","iopub.execute_input":"2024-05-05T12:40:17.553547Z","iopub.status.idle":"2024-05-05T12:40:17.572013Z","shell.execute_reply.started":"2024-05-05T12:40:17.553515Z","shell.execute_reply":"2024-05-05T12:40:17.570961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.predict(query_point_creator(q1,q4))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:40:26.731991Z","iopub.execute_input":"2024-05-05T12:40:26.732433Z","iopub.status.idle":"2024-05-05T12:40:26.750064Z","shell.execute_reply.started":"2024-05-05T12:40:26.7324Z","shell.execute_reply":"2024-05-05T12:40:26.748891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.predict(query_point_creator(q2,q3))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:40:38.59617Z","iopub.execute_input":"2024-05-05T12:40:38.597107Z","iopub.status.idle":"2024-05-05T12:40:38.613425Z","shell.execute_reply.started":"2024-05-05T12:40:38.597067Z","shell.execute_reply":"2024-05-05T12:40:38.612194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.predict(query_point_creator('This tastes good','its tasting good'))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:47:46.089955Z","iopub.execute_input":"2024-05-05T12:47:46.091101Z","iopub.status.idle":"2024-05-05T12:47:46.112662Z","shell.execute_reply.started":"2024-05-05T12:47:46.091057Z","shell.execute_reply":"2024-05-05T12:47:46.111096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pickle\n\npickle.dump(rf,open('model.pkl','wb'))\npickle.dump(cv,open('cv.pkl','wb'))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:38:55.769192Z","iopub.execute_input":"2024-05-05T12:38:55.769972Z","iopub.status.idle":"2024-05-05T12:38:55.898302Z","shell.execute_reply.started":"2024-05-05T12:38:55.769941Z","shell.execute_reply":"2024-05-05T12:38:55.897255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}